{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forking Experiments\n",
    "\n",
    "This notebook contains some experimental results from running several network simulations that can help us to understand how some settings and parameters affect the network's throughput (more specifically on the chain forks evolution) under a Proof of Stake v3 proposing mechanism.\n",
    "\n",
    "But before showing the experimental results, we'll introduce some basic calculations to see what the theory tells us and have a better idea of which kind of questions we're trying to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplest case: 0-delay\n",
    "\n",
    "Given any arbitrary target spacing (average time between blocks), which in our case is 16s, and 0 delay, two variables control the probability of having forks:\n",
    "\n",
    "  - The numer of staked coins.\n",
    "  - The time granularity used to compute block hashes.\n",
    "\n",
    "**Some assumptions:**\n",
    "\n",
    "  - There's no delay, once a block is created, it arrives immediately to all the nodes in the network.\n",
    "    Although it's intuitive to expect that higher delays could lead to higher number of forks, we'll\n",
    "    assume for now that there's no delay to have a better intuition on how other factors could affect\n",
    "    the outcome.\n",
    "  - All coins have the same denomination and each proposer holds one single coin, this is a strong (and not realistic) assumption, but not very problematic.\n",
    "  - The difficulty adjustment mechanism is good enough to keep empirical probabilities close enough to the desired ones.\n",
    "  - Given that the delay is 0, and that we have a fork choice rule, the \"weakest\" forks are rapidly discarded and the nodes don't try to build chains on top of them.\n",
    "\n",
    "**Some definitions:**\n",
    "\n",
    "  - $C$ : number of staked coins\n",
    "  - $T$ : target spacing, or expected average time between blocks\n",
    "  - $m$ : in $T$ seconds a node will be able to try $m$ hashes.\n",
    "  - $P\\left(C,m,k\\right)$ : Probability that a coin gives us the ability to propose during a period of $k\\frac{T}{m}$ seconds when there are $C$ staked coins, the target spacing is $T$ and a node can try to propose every $\\frac{T}{m}$ seconds.\n",
    "  - $\\mathsf{P}$ : $P\\left(C,m,1\\right)$.\n",
    "  - $F\\left(C,m,k\\right)$ : Probability of having at least one fork during a period of $k\\frac{T}{m}$ seconds when there are $C$ staked coins, the target spacing is $T$ and a node can try to propose every $\\frac{T}{m}$ seconds.\n",
    "  - $\\mathsf{F}$ : $F\\left(C,m,1\\right)$.\n",
    "  - $\\mathbb{F}$ : $F\\left(C,m,m\\right)$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some calculations:**\n",
    "\n",
    "The probability of having at least one fork for a given instant is $1$ minus the probability of not having any block proposal, minus the probability of having exactly $1$ block proposal:\n",
    "\n",
    "$$\\mathsf{F} = 1 - \\mathsf{P}\\left(1 - \\mathsf{P}\\right)^{C-1} - \\left(1 - \\mathsf{P}\\right)^C = 1 - \\left(1 - \\mathsf{P}\\right)^{C-1}$$\n",
    "\n",
    "The probability of having at least one fork during $T$ seconds is:\n",
    "\n",
    "$$\\mathbb{F} = 1 - \\left(1-\\mathsf{F}\\right)^m$$\n",
    "\n",
    "We want to find expressions that depend on $C$ and $m$, so let's start doing some substitions:\n",
    "\n",
    "$$\\frac{1}{m} = 1 - \\left(1-\\mathsf{P}\\right)^C \\implies \\mathsf{P} = 1-\\left(1-\\frac{1}{m}\\right)^{\\frac{1}{C}}$$\n",
    "\n",
    "and hence\n",
    "\n",
    "$$\\mathsf{F} = 1 - \\left(1-\\mathsf{P}\\right)^{C-1} = 1 - \\left( 1 - \\left( 1 - \\left( 1 - \\frac{1}{m}\\right)^{\\frac{1}{C}}\\right)\\right)^{C-1} = 1 - \\left(1-\\frac{1}{m}\\right)^{\\frac{C-1}{C}}$$\n",
    "\n",
    "therefore\n",
    "\n",
    "$$\\mathbb{F} = 1 - \\left(1-\\mathsf{F}\\right)^m = 1 - \\left( 1 - \\left( 1 - \\left( 1 - \\frac{1}{m}\\right)^{\\frac{C-1}{C}}\\right)\\right)^m = 1 - \\left(1-\\frac{1}{m}\\right)^{m\\frac{C-1}{C}}$$\n",
    "\n",
    "As we can see, when the number of coins ($C$) is big enough, the probability of having at least one fork every $T$ seconds is $1-\\left(1-\\frac{1}{m}\\right)^m$, which is a decreasing function of $m$; and $\\lim_{m\\to\\infty}1-\\left(1-\\frac{1}{m}\\right)^m = 1-e^{-1}$.\n",
    "\n",
    "We can take a look on $\\mathbb{F}$ seeing what happens when $m$ is very large. The probability of having forks every $T$ seconds is $1-e^{-1+\\frac{1}{C}}$, which makes sense: the more proposers, the higher the probability of having forks (and having just one proposer gives us a probability equal to $0$).\n",
    "\n",
    "From this we know that, having negligible delay, it's worth to have a large $m$ parameter, but we still face a fundamental limit that we can't cross, some amount of forking is inevitable. This tells us that, having a target spacing of 16s we'll see a fork approximately every 25.31s in average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A more convoluted case: deterministic $\\delta$-delay\n",
    "\n",
    "Now let's see what happens when the block propagation delay is positive, although this is more realistic, we'll still keep things simple by assuming a deterministic fixed delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we use part of Unit-e's functional tests framework, we have to set some global properties\n",
    "import test_framework.util as tf_util\n",
    "\n",
    "tf_util.MAX_NODES = 500  # has to be greater than 2n+2 where n = num_nodes\n",
    "tf_util.PortSeed.n = 314159  # We want reproducible pseudo-random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do this to make easier mixing sync & async code inside Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports required to run our experiments\n",
    "import pandas as pd\n",
    "\n",
    "from asyncio import get_event_loop\n",
    "from itertools import product as cartesian_product \n",
    "from pathlib import Path\n",
    "\n",
    "from experiments.forking_simulation import ForkingSimulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some constants for our experiments\n",
    "simulation_time = 30 # TODO: Change it to be 600s\n",
    "sample_time = 1      # 1s\n",
    "graph_model = 'preferential_attachment'\n",
    "\n",
    "# We'll have to store the results of our simulations somewhere\n",
    "current_path = Path('.').resolve()\n",
    "results_path = current_path.joinpath('..').joinpath('results').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll combine these values to generate many different settings for our simulations\n",
    "num_nodes_values = [50]                  # How many nodes we'll have in our network\n",
    "num_proposer_nodes_values = [5]          # How many nodes will be able to propose blocks\n",
    "target_spacings = [16]                   # Expected averate time between blocks\n",
    "time_steps = [1]                         # Time granularity used to generate block hashes\n",
    "latencies = [0]                          # Block propagation delays\n",
    "\n",
    "settings_tuples = list(cartesian_product(\n",
    "    num_nodes_values,\n",
    "    num_proposer_nodes_values,\n",
    "    target_spacings,\n",
    "    time_steps,\n",
    "    latencies\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data will be generated for 1 settings tuples.\n",
      "Generating data will require 40 seconds, or 0.011111111111111112 hours.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's show a summary to know how much we'll have to wait\n",
    "num_settings = len(settings_tuples)\n",
    "expected_time = num_settings * (10 + simulation_time)\n",
    "print(f'\\n\\nData will be generated for {num_settings} settings tuples.')\n",
    "print(f'Generating data will require {expected_time} seconds, or {expected_time/3600} hours.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "430bce11c3404fdebda5fb75cf591f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='0 / 1'), IntProgress(value=0, max=1)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In this cell we'll generate data in order to analyze it later\n",
    "\n",
    "from ipywidgets import IntProgress, HTML, VBox\n",
    "from IPython.display import display\n",
    "\n",
    "# We'll show a progress bar to haver a better sense of progress\n",
    "progress_bar = IntProgress(min=0, max=num_settings)\n",
    "label = HTML()\n",
    "label.value = f'0 / {num_settings}'\n",
    "box = VBox(children=[label, progress_bar])\n",
    "display(box)\n",
    "\n",
    "# We generate a dataset per settings tuple\n",
    "for c, settings in enumerate(settings_tuples):\n",
    "    num_nodes, num_proposer_nodes, target_spacing, time_step, latency = settings\n",
    "    \n",
    "    # Each simulation stores results in a different path\n",
    "    result_directory = results_path.joinpath(\n",
    "        '_'.join(str(v) for v in settings)\n",
    "    ).resolve()\n",
    "    \n",
    "    # Previous data will be overwritten\n",
    "    if result_directory.exists():\n",
    "        for f in result_directory.glob(\"*.csv\"):\n",
    "            f.unlink()\n",
    "        result_directory.rmdir()\n",
    "    result_directory.mkdir(exist_ok=True)\n",
    "    \n",
    "    network_stats_filename = str(result_directory.joinpath('network.csv').resolve())\n",
    "    nodes_stats_directory = result_directory\n",
    "    \n",
    "    simulation = ForkingSimulation(\n",
    "        loop=get_event_loop(),\n",
    "        latency=latency,\n",
    "        num_proposer_nodes=num_proposer_nodes,\n",
    "        num_relay_nodes=num_nodes - num_proposer_nodes,\n",
    "        simulation_time=simulation_time,\n",
    "        sample_time=sample_time,\n",
    "        graph_model=graph_model,\n",
    "        block_time_seconds=target_spacing,\n",
    "        block_stake_timestamp_interval_seconds=time_step,\n",
    "        network_stats_file_name=network_stats_filename,\n",
    "        nodes_stats_directory=nodes_stats_directory\n",
    "    )\n",
    "    simulation.safe_run(close_loop=False)\n",
    "    \n",
    "    # Updating the progress bar\n",
    "    progress_bar.value += 1\n",
    "    label.value = f'{progress_bar.value} / {num_settings}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to load all the generated data into memory to draw some conclusions\n",
    "\n",
    "# We'll merge all the stats collected by different nodes (and different settings) into one single dataframe\n",
    "nodes_df = None\n",
    "network_df = None\n",
    "\n",
    "for settings in settings_tuples:\n",
    "    num_nodes, num_proposer_nodes, target_spacing, time_step, latency = settings\n",
    "\n",
    "    result_directory = results_path.joinpath(\n",
    "        '_'.join(str(v) for v in settings)\n",
    "    ).resolve()\n",
    "    network_stats_filename = str(result_directory.joinpath('network.csv').resolve())\n",
    "\n",
    "    # timestamps are measured in milliseconds\n",
    "    network_csv = pd.read_csv(\n",
    "        network_stats_filename,\n",
    "        header=None,\n",
    "        names=['timestamp', 'src_id', 'dst_id', 'command', 'command_size']\n",
    "    )\n",
    "    network_csv['num_nodes'] = num_nodes\n",
    "    network_csv['num_proposer_nodes'] = num_proposer_nodes\n",
    "    network_csv['target_spacing'] = target_spacing\n",
    "    network_csv['time_step'] = time_step\n",
    "    network_csv['latency'] = latency\n",
    "    if network_df is None:\n",
    "        network_df = network_csv\n",
    "    else:\n",
    "        network_df = pd.concat([network_df, network_csv])\n",
    "\n",
    "    for node_id in range(num_nodes):\n",
    "        node_stats_filename = str(result_directory.joinpath(f'stats_{node_id}.csv').resolve())\n",
    "        \n",
    "        node_csv = pd.read_csv(\n",
    "            node_stats_filename,\n",
    "            header=None,\n",
    "            names=[\n",
    "                'timestamp', 'height', 'mempool_num_transactions', 'mempool_used_memory',\n",
    "                'peers_num_inbound', 'peers_num_outbound', 'tip_stats_active', 'tip_stats_valid_fork',\n",
    "                'tip_stats_valid_header', 'tip_stats_headers_only', 'tip_stats_invalid'\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # We add some columns to identify the source node & the specific simulation\n",
    "        node_csv['node_id'] = node_id\n",
    "        node_csv['num_nodes'] = num_nodes\n",
    "        node_csv['num_proposer_nodes'] = num_proposer_nodes\n",
    "        node_csv['target_spacing'] = target_spacing\n",
    "        node_csv['time_step'] = time_step\n",
    "        node_csv['latency'] = latency\n",
    "        \n",
    "        node_csv = node_csv.reindex(columns=[\n",
    "            'timestamp',\n",
    "            'node_id', 'num_nodes', 'num_proposer_nodes', 'target_spacing', 'time_step', 'latency',\n",
    "            'height', 'mempool_num_transactions', 'mempool_used_memory',\n",
    "            'peers_num_inbound', 'peers_num_outbound', 'tip_stats_active', 'tip_stats_valid_fork',\n",
    "            'tip_stats_valid_header', 'tip_stats_headers_only', 'tip_stats_invalid'\n",
    "        ])\n",
    "        \n",
    "        if nodes_df is None:\n",
    "            nodes_df = node_csv\n",
    "        else:\n",
    "            nodes_df = pd.concat([nodes_df, node_csv])\n",
    "            nodes_df = nodes_df.sort_values(by=['timestamp', 'node_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "nteract": {
   "version": "nteract-on-jupyter@2.0.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
